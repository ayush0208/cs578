{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data.csv')\n",
    "data.drop('Unnamed: 0', 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spoiled rich kid kelley morse ( chris klein ) ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the bond series is an island in the film world...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tarzan chad'z = good ) 1999 , g , 90 minutes [...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a frequent error is the categorization of a te...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>part one of \" the strangest movies ever made \"...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1695</th>\n",
       "      <td>it is hard to imagine that a movie which inclu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1696</th>\n",
       "      <td>there was a time when john carpenter was a gre...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1697</th>\n",
       "      <td>sam ( matthew broderick ) is an astronomer in ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1698</th>\n",
       "      <td>this is my first review that i post to this ne...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1699</th>\n",
       "      <td>whenever u . s . government starts meddling in...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1700 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text  Label\n",
       "0     spoiled rich kid kelley morse ( chris klein ) ...      0\n",
       "1     the bond series is an island in the film world...      1\n",
       "2     tarzan chad'z = good ) 1999 , g , 90 minutes [...      1\n",
       "3     a frequent error is the categorization of a te...      1\n",
       "4     part one of \" the strangest movies ever made \"...      1\n",
       "...                                                 ...    ...\n",
       "1695  it is hard to imagine that a movie which inclu...      1\n",
       "1696  there was a time when john carpenter was a gre...      0\n",
       "1697  sam ( matthew broderick ) is an astronomer in ...      1\n",
       "1698  this is my first review that i post to this ne...      0\n",
       "1699  whenever u . s . government starts meddling in...      0\n",
       "\n",
       "[1700 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\garga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_count(text):\n",
    "  word_count = {}\n",
    "  for sent in sent_tokenize(text):\n",
    "    for word in word_tokenize(sent):\n",
    "      if word not in word_count:\n",
    "        word_count[word] = 0\n",
    "      word_count[word] +=1\n",
    "  return word_count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "  return [word for sent in sent_tokenize(text) for word in word_tokenize(sent)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "def remove_stopwords(text):\n",
    "    words = [word for word in tokenize_text(text).keys() if word.lower() not in stopwords_list]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\n",
    "for idx, row in data.iterrows():\n",
    "    text = row['Text']\n",
    "    corpus += text + \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.PorterStemmer()\n",
    "def replace_text_stemming(text):\n",
    "  stems = [stemmer.stem(word) for word in tokenize_text(text)]\n",
    "  return \" \".join(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_corpus = replace_text_stemming(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30480\n",
      "4690\n"
     ]
    }
   ],
   "source": [
    "vocabulary = get_word_count(stemmed_corpus)\n",
    "print(len(vocabulary))\n",
    "reduced_vocab = []\n",
    "for key in vocabulary:\n",
    "    if vocabulary[key] > 16 and key not in stopwords_list:\n",
    "        reduced_vocab.append(key)\n",
    "print(len(reduced_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'c' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-d962e294eb86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"k\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: 'c' is not in list"
     ]
    }
   ],
   "source": [
    "ex = [\"i\", \"k\"]\n",
    "ex.index(\"c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(text, vocabulary):\n",
    "    tokens = tokenize_text(text)\n",
    "    vector = np.zeros(len(vocabulary))\n",
    "    for token in tokens:\n",
    "        if token in vocabulary:\n",
    "            vector[vocabulary.index(token)]+= 1\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 2., 1., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_vector(data.loc[0]['Text'], reduced_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vnarapar/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "sw_nltk = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(sw_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spoiled',\n",
       " 'rich',\n",
       " 'kid',\n",
       " 'kelley',\n",
       " '(',\n",
       " 'chris',\n",
       " ')',\n",
       " 'receives',\n",
       " 'a',\n",
       " 'new',\n",
       " 'for',\n",
       " 'present',\n",
       " '.',\n",
       " 'he',\n",
       " 'and',\n",
       " 'his',\n",
       " 'buddies',\n",
       " 'take',\n",
       " 'it',\n",
       " 'to',\n",
       " 'small',\n",
       " 'nearby',\n",
       " 'town',\n",
       " ',',\n",
       " 'where',\n",
       " 'proceeds',\n",
       " 'the',\n",
       " 'simply',\n",
       " 'because',\n",
       " \"'s\",\n",
       " 'they',\n",
       " \"'re\",\n",
       " 'not',\n",
       " 'ends',\n",
       " 'up',\n",
       " 'provoking',\n",
       " 'josh',\n",
       " 'into',\n",
       " 'race',\n",
       " 'as',\n",
       " 'result',\n",
       " 'local',\n",
       " 'gas',\n",
       " 'station',\n",
       " 'diner',\n",
       " 'are',\n",
       " 'destroyed',\n",
       " 'when',\n",
       " 'crash',\n",
       " 'is',\n",
       " 'has',\n",
       " 'live',\n",
       " 'with',\n",
       " 'in',\n",
       " 'spare',\n",
       " 'room',\n",
       " 'over',\n",
       " 'family',\n",
       " 'girlfriend',\n",
       " 'sam',\n",
       " 'soon',\n",
       " 'takes',\n",
       " 'liking',\n",
       " 'however',\n",
       " 'despite',\n",
       " 'fact',\n",
       " 'that',\n",
       " '1',\n",
       " 'was',\n",
       " 'her',\n",
       " 'b',\n",
       " 'all',\n",
       " 'does',\n",
       " 'sit',\n",
       " 'around',\n",
       " 'smart',\n",
       " 'off',\n",
       " 'but',\n",
       " 'she',\n",
       " 'sees',\n",
       " 'him',\n",
       " 'apparently',\n",
       " 'needs',\n",
       " 'lose',\n",
       " 'feelings',\n",
       " 'whose',\n",
       " 'only',\n",
       " 'fault',\n",
       " 'seems',\n",
       " 'be',\n",
       " 'case',\n",
       " 'of',\n",
       " 'hat',\n",
       " 'hair',\n",
       " 'so',\n",
       " 'falls',\n",
       " 'love',\n",
       " 'especially',\n",
       " 'after',\n",
       " 'day',\n",
       " 'follows',\n",
       " 'through',\n",
       " 'woods',\n",
       " 'hears',\n",
       " 'giving',\n",
       " 'speech',\n",
       " 'which',\n",
       " 'unable',\n",
       " 'deliver',\n",
       " 'caused',\n",
       " 'destruction',\n",
       " 'property',\n",
       " 'nearly',\n",
       " 'killed',\n",
       " 'dozens',\n",
       " 'people',\n",
       " 'by',\n",
       " 'robert',\n",
       " 'just',\n",
       " 'happens',\n",
       " 'favorite',\n",
       " 'well',\n",
       " 'fields',\n",
       " 'some',\n",
       " 'reason',\n",
       " 'waste',\n",
       " 'no',\n",
       " 'time',\n",
       " 'letting',\n",
       " 'best',\n",
       " 'friend',\n",
       " 'see',\n",
       " 'them',\n",
       " 'together',\n",
       " 'continues',\n",
       " 'make',\n",
       " 'an',\n",
       " 'ass',\n",
       " 'himself',\n",
       " 'shows',\n",
       " 'at',\n",
       " 'dance',\n",
       " 'drunk',\n",
       " 'went',\n",
       " 'runs',\n",
       " 'decides',\n",
       " 'leave',\n",
       " 'catches',\n",
       " 'bus',\n",
       " 'asks',\n",
       " 'come',\n",
       " 'boston',\n",
       " 'how',\n",
       " 'much',\n",
       " 'loves',\n",
       " 'without',\n",
       " 'even',\n",
       " 'breaking',\n",
       " 'basically',\n",
       " 'says',\n",
       " '``',\n",
       " '!',\n",
       " 'here',\n",
       " 'on',\n",
       " 'earth',\n",
       " 'goes',\n",
       " 'many',\n",
       " 'wrong',\n",
       " 'directions',\n",
       " 'from',\n",
       " 'start',\n",
       " 'its',\n",
       " 'fairly',\n",
       " 'decent',\n",
       " 'ending',\n",
       " 'comes',\n",
       " 'completely',\n",
       " 'worthless',\n",
       " 'two',\n",
       " 'my',\n",
       " 'current',\n",
       " 'young',\n",
       " 'performers',\n",
       " 'their',\n",
       " 'presence',\n",
       " 'i',\n",
       " 'watched',\n",
       " 'this',\n",
       " 'film',\n",
       " 'first',\n",
       " 'place',\n",
       " 'long',\n",
       " 'proceedings',\n",
       " 'wanted',\n",
       " 'reach',\n",
       " 'both',\n",
       " 'should',\n",
       " \"n't\",\n",
       " 'have',\n",
       " 'such',\n",
       " 'towards',\n",
       " 'like',\n",
       " 'characters',\n",
       " 'trust',\n",
       " 'me',\n",
       " 'you',\n",
       " \"'ll\",\n",
       " 'feel',\n",
       " 'same',\n",
       " 'way',\n",
       " 'though',\n",
       " 'lead',\n",
       " 'male',\n",
       " 'usually',\n",
       " 'hands',\n",
       " 'down',\n",
       " 'more',\n",
       " 'actions',\n",
       " 'each',\n",
       " 'every',\n",
       " 'other',\n",
       " 'hand',\n",
       " 'afterwards',\n",
       " 'across',\n",
       " 'being',\n",
       " 'truly',\n",
       " 'sorry',\n",
       " 'life',\n",
       " 'nice',\n",
       " 'guys',\n",
       " 'finish',\n",
       " 'last',\n",
       " 'entire',\n",
       " 'we',\n",
       " 'watch',\n",
       " 'fall',\n",
       " 'guy',\n",
       " 'one',\n",
       " 'likes',\n",
       " 'while',\n",
       " 'sympathetic',\n",
       " 'character',\n",
       " 'gets',\n",
       " 'then',\n",
       " 'subplot',\n",
       " 'play',\n",
       " 'things',\n",
       " 'get',\n",
       " 'if',\n",
       " 'can',\n",
       " 'imagine',\n",
       " 'what',\n",
       " 'were',\n",
       " 'depressing',\n",
       " 'enough',\n",
       " 'mr',\n",
       " 'screenwriter',\n",
       " '?',\n",
       " 'who',\n",
       " 'project',\n",
       " 'thought',\n",
       " 'would',\n",
       " 'entertain',\n",
       " 'anyone',\n",
       " 'perhaps',\n",
       " 'had',\n",
       " 'been',\n",
       " 'felt',\n",
       " 'these',\n",
       " 'lives',\n",
       " 'good',\n",
       " 'measure',\n",
       " 'available',\n",
       " 'dvd',\n",
       " 'fox',\n",
       " 'home',\n",
       " 'entertainment',\n",
       " 'contains',\n",
       " 'original',\n",
       " 'theatrical',\n",
       " 'aspect',\n",
       " 'ratio',\n",
       " ':',\n",
       " 'enhanced',\n",
       " 'extras',\n",
       " 'include',\n",
       " 'jessica',\n",
       " 'music',\n",
       " 'video',\n",
       " 'commercial',\n",
       " 'soundtrack',\n",
       " 'thing',\n",
       " 'about',\n",
       " 'trailer',\n",
       " 'five',\n",
       " 'tv',\n",
       " 'spots',\n",
       " 'trailers',\n",
       " 'releases',\n",
       " 'anna',\n",
       " 'king',\n",
       " 'anywhere',\n",
       " 'beach',\n",
       " 'drive',\n",
       " 'crazy',\n",
       " 'ever',\n",
       " 'romeo',\n",
       " 'juliet',\n",
       " 'irresistible',\n",
       " 'upcoming',\n",
       " 'really',\n",
       " 'enjoy',\n",
       " 'picture',\n",
       " 'sound',\n",
       " 'quality',\n",
       " 'disc',\n",
       " 'fine',\n",
       " 'nothing',\n",
       " 'will',\n",
       " 'challenge',\n",
       " 'course',\n",
       " 'fans',\n",
       " 'movie',\n",
       " 'any',\n",
       " 'exist',\n",
       " 'release',\n",
       " 'entertaining',\n",
       " 'nor',\n",
       " 'compelling',\n",
       " 'joy',\n",
       " 'or',\n",
       " 'value',\n",
       " 'events',\n",
       " 'merely',\n",
       " 'money',\n",
       " 'talent',\n",
       " 'bond',\n",
       " 'series',\n",
       " 'island',\n",
       " 'world',\n",
       " ';',\n",
       " 'else',\n",
       " 'look',\n",
       " 'forward',\n",
       " 'cliches',\n",
       " 'occur',\n",
       " 'most',\n",
       " 'films',\n",
       " 'pure',\n",
       " 'vulnerable',\n",
       " 'hero',\n",
       " 'talking',\n",
       " 'villains',\n",
       " 'blatant',\n",
       " 'product',\n",
       " 'predictable',\n",
       " 'action',\n",
       " 'sequences',\n",
       " 'lots',\n",
       " 'hoped',\n",
       " 'instead',\n",
       " 'significant',\n",
       " 'change',\n",
       " 'do',\n",
       " 'understand',\n",
       " 'myself',\n",
       " 'based',\n",
       " 'think',\n",
       " '18',\n",
       " \"'ve\",\n",
       " 'mostly',\n",
       " 'got',\n",
       " 'again',\n",
       " 'variation',\n",
       " 'bit',\n",
       " 'notably',\n",
       " 'ca',\n",
       " 'bitch',\n",
       " 'tomorrow',\n",
       " 'never',\n",
       " 'dies',\n",
       " 'second',\n",
       " 'brosnan',\n",
       " 'exception',\n",
       " 'thank',\n",
       " 'god',\n",
       " 'plot',\n",
       " 'kind',\n",
       " 'edge',\n",
       " 'plain',\n",
       " 'media',\n",
       " '-',\n",
       " 'elliot',\n",
       " 'carver',\n",
       " 'jonathan',\n",
       " 'looking',\n",
       " 'cut',\n",
       " 'always',\n",
       " 'out',\n",
       " 'breath',\n",
       " 'idea',\n",
       " 'getting',\n",
       " 'ratings',\n",
       " 'creating',\n",
       " 'war',\n",
       " 'wag',\n",
       " 'dog',\n",
       " 'wants',\n",
       " 'real',\n",
       " 'between',\n",
       " 'china',\n",
       " 'england',\n",
       " 'stupid',\n",
       " 'put',\n",
       " 'before',\n",
       " 'discovered',\n",
       " 'james',\n",
       " 'sent',\n",
       " 'trying',\n",
       " 'research',\n",
       " 'fighting',\n",
       " 'men',\n",
       " 'running',\n",
       " 'than',\n",
       " 'occasion',\n",
       " 'chinese',\n",
       " 'secret',\n",
       " 'agent',\n",
       " 'michelle',\n",
       " 'asian',\n",
       " 'star',\n",
       " 'co-star',\n",
       " 'stealing',\n",
       " 'screen',\n",
       " 'jackie',\n",
       " 'chan',\n",
       " 'eventually',\n",
       " 'predictably',\n",
       " 'begin',\n",
       " 'working',\n",
       " 'yes',\n",
       " 'there',\n",
       " 'sexual',\n",
       " 'tension',\n",
       " 'luckily',\n",
       " 'relationship',\n",
       " 'gives',\n",
       " 'until',\n",
       " 'finale',\n",
       " 'average',\n",
       " 'three',\n",
       " 'women',\n",
       " 'sometimes',\n",
       " 'less',\n",
       " 'latter',\n",
       " 'potent',\n",
       " 'spoil',\n",
       " 'mean',\n",
       " 'need',\n",
       " 'stay',\n",
       " 'pg-13',\n",
       " 'rating',\n",
       " 'beginning',\n",
       " 'chick',\n",
       " 'big',\n",
       " 'girl',\n",
       " 'wife',\n",
       " 'paris',\n",
       " 'once',\n",
       " 'girls',\n",
       " 'very',\n",
       " 'attractive',\n",
       " 'dull',\n",
       " 'part',\n",
       " 'oh',\n",
       " 'dirty',\n",
       " 'work',\n",
       " 'tall',\n",
       " 'mention',\n",
       " 'shaw',\n",
       " 'named',\n",
       " 'pretty',\n",
       " 'awesome',\n",
       " 'although',\n",
       " 'stiff',\n",
       " 'comedy',\n",
       " 'cameo',\n",
       " 'vincent',\n",
       " 'german',\n",
       " 'assassin',\n",
       " 'point',\n",
       " 'government',\n",
       " 'parts',\n",
       " 'usual',\n",
       " 'near',\n",
       " 'death',\n",
       " 'sadly',\n",
       " 'scene',\n",
       " 'mood',\n",
       " 'm',\n",
       " 'whatsoever',\n",
       " 'except',\n",
       " 'joe',\n",
       " 'don',\n",
       " 'baker',\n",
       " 'annoying',\n",
       " 'go',\n",
       " 'too',\n",
       " 'pop',\n",
       " 'now',\n",
       " 'mainly',\n",
       " 'sake',\n",
       " 'ones',\n",
       " 'opening',\n",
       " 'remote',\n",
       " 'control',\n",
       " 'chase',\n",
       " 'nonetheless',\n",
       " 'streets',\n",
       " 'vietnam',\n",
       " 'essential',\n",
       " 'feature',\n",
       " 'laughs',\n",
       " 'yeah',\n",
       " 'dig',\n",
       " 'distracting',\n",
       " 'supposed',\n",
       " 'acting',\n",
       " 'great',\n",
       " 'actually',\n",
       " 'better',\n",
       " 'unique',\n",
       " 'independent',\n",
       " 'certain',\n",
       " 'married',\n",
       " 'end',\n",
       " 'service',\n",
       " 'added',\n",
       " 'emotional',\n",
       " 'depth',\n",
       " 'notable',\n",
       " 'since',\n",
       " 'chemistry',\n",
       " 'villain',\n",
       " 'lovable',\n",
       " 'daddy',\n",
       " 'built',\n",
       " 'fun',\n",
       " 'add',\n",
       " 'remember',\n",
       " 'years',\n",
       " 'serious',\n",
       " 'direction',\n",
       " 'themes',\n",
       " 'characterization',\n",
       " 'yet',\n",
       " \"'m\",\n",
       " 'bigger',\n",
       " 'united',\n",
       " 'states',\n",
       " 'having',\n",
       " 'seen',\n",
       " 'twice',\n",
       " 'already',\n",
       " 'tell',\n",
       " 'installment',\n",
       " 'fully',\n",
       " 'amazing',\n",
       " 'tarzan',\n",
       " 'chad',\n",
       " \"'\",\n",
       " 'z',\n",
       " '=',\n",
       " '1999',\n",
       " 'g',\n",
       " '90',\n",
       " 'minutes',\n",
       " '[',\n",
       " 'hour',\n",
       " '30',\n",
       " ']',\n",
       " 'animated',\n",
       " 'starring',\n",
       " 'voices',\n",
       " 'tony',\n",
       " 'minnie',\n",
       " 'driver',\n",
       " 'jane',\n",
       " 'porter',\n",
       " 'lance',\n",
       " 'brian',\n",
       " 'clayton',\n",
       " 'written',\n",
       " 'murphy',\n",
       " 'bob',\n",
       " 'white',\n",
       " 'produced',\n",
       " 'arnold',\n",
       " 'directed',\n",
       " 'buck',\n",
       " 'kevin',\n",
       " 'story',\n",
       " '`',\n",
       " 'apes',\n",
       " 'created',\n",
       " 'july',\n",
       " '4',\n",
       " '7',\n",
       " '15',\n",
       " 'p',\n",
       " 'theater',\n",
       " '#',\n",
       " '9',\n",
       " 'free',\n",
       " 'using',\n",
       " 'season',\n",
       " 'pass',\n",
       " '*',\n",
       " 'seats',\n",
       " 'title',\n",
       " 'beautiful',\n",
       " 'easy',\n",
       " 'follow',\n",
       " 'review',\n",
       " 'disney',\n",
       " 'trend',\n",
       " 'producing',\n",
       " 'summer',\n",
       " 'blockbuster',\n",
       " 'movies',\n",
       " 'meant',\n",
       " 'children',\n",
       " 'wit',\n",
       " 'charm',\n",
       " 'adults',\n",
       " 'interested',\n",
       " 'least',\n",
       " 'entertained',\n",
       " 'basic',\n",
       " 'premise',\n",
       " 'boy',\n",
       " 'raised',\n",
       " 'african',\n",
       " 'jungle',\n",
       " 'becomes',\n",
       " 'understanding',\n",
       " 'human',\n",
       " 'lot',\n",
       " 'different',\n",
       " 'animals',\n",
       " 'befriends',\n",
       " 'third',\n",
       " 'voiced',\n",
       " 'glen',\n",
       " 'close',\n",
       " 'female',\n",
       " 'gorilla',\n",
       " 'lost',\n",
       " 'own',\n",
       " 'child',\n",
       " 'classic',\n",
       " 'mate',\n",
       " 'leader',\n",
       " 'pack',\n",
       " 'extended',\n",
       " 'refuses',\n",
       " 'alex',\n",
       " 'd',\n",
       " 'during',\n",
       " 'childhood',\n",
       " 'scenes',\n",
       " 'son',\n",
       " 'often',\n",
       " 'hold',\n",
       " 'back',\n",
       " 'killing',\n",
       " 'continually',\n",
       " 'dangerous',\n",
       " 'wild',\n",
       " 'instincts',\n",
       " 'feeling',\n",
       " 'quickly',\n",
       " 'able',\n",
       " 'identify',\n",
       " 'funny',\n",
       " 'fail',\n",
       " 'later',\n",
       " 'man',\n",
       " 'encounter',\n",
       " 'party',\n",
       " 'humans',\n",
       " 'woman',\n",
       " 'something',\n",
       " 'artist',\n",
       " 'father',\n",
       " 'also',\n",
       " 'scientist',\n",
       " 'slick',\n",
       " 'going',\n",
       " 'times',\n",
       " 'screenplay',\n",
       " 'seem',\n",
       " 'enjoyment',\n",
       " 'sure',\n",
       " 'cliche',\n",
       " 'stuff',\n",
       " 'manipulation',\n",
       " 'goofy',\n",
       " 'supporting',\n",
       " 'rosie',\n",
       " \"o'donnell\",\n",
       " 'full',\n",
       " 'brooklyn',\n",
       " 'accent',\n",
       " 'wayne',\n",
       " 'knight',\n",
       " 'newman',\n",
       " 'sliding',\n",
       " 'tree',\n",
       " 'fast',\n",
       " 'doing',\n",
       " 'sorts',\n",
       " 'tricks',\n",
       " 'those',\n",
       " 'mcdonald',\n",
       " 'commercials',\n",
       " 'prove',\n",
       " 'ultimately',\n",
       " 'exactly',\n",
       " 'expect',\n",
       " 'high',\n",
       " 'expectations',\n",
       " 'animation',\n",
       " 'senses',\n",
       " 'songs',\n",
       " 'stuck',\n",
       " 'your',\n",
       " 'head',\n",
       " 'surprisingly',\n",
       " 'violent',\n",
       " 'rated',\n",
       " 'pg',\n",
       " 'may',\n",
       " 'adult',\n",
       " 'parents',\n",
       " 'ready',\n",
       " 'heavy',\n",
       " 'questions',\n",
       " 'thrown',\n",
       " 'short',\n",
       " 'recommend',\n",
       " 'friends',\n",
       " 'public',\n",
       " 'score',\n",
       " '0',\n",
       " 'frequent',\n",
       " 'terrorist',\n",
       " 'soldier',\n",
       " 'common',\n",
       " 'criminal',\n",
       " 'commits',\n",
       " 'acts',\n",
       " 'violence',\n",
       " 'against',\n",
       " 'another',\n",
       " 'personal',\n",
       " 'gain',\n",
       " 'random',\n",
       " 'means',\n",
       " 'political',\n",
       " 'target',\n",
       " 'nation',\n",
       " 'desired',\n",
       " 'fear',\n",
       " 'resulting',\n",
       " 'strike',\n",
       " 'total',\n",
       " 'figure',\n",
       " 'edward',\n",
       " 'siege',\n",
       " 'explores',\n",
       " 'possibility',\n",
       " 'taking',\n",
       " 'right',\n",
       " \"'d\",\n",
       " 'ripped',\n",
       " 'terrorists',\n",
       " 'kill',\n",
       " 'american',\n",
       " 'citizens',\n",
       " 'u',\n",
       " 's',\n",
       " 'whom',\n",
       " 'supposedly',\n",
       " 'attack',\n",
       " 'unlike',\n",
       " 'real-life',\n",
       " 'cruise',\n",
       " 'camp',\n",
       " 'version',\n",
       " 'special',\n",
       " 'troops',\n",
       " 'kidnapping',\n",
       " 'bring',\n",
       " 'fight',\n",
       " 'involved',\n",
       " 'fbi',\n",
       " 'assistant',\n",
       " 'anthony',\n",
       " 'denzel',\n",
       " 'washington',\n",
       " 'works',\n",
       " 'york',\n",
       " 'office',\n",
       " 'federal',\n",
       " 'investigating',\n",
       " 'apple',\n",
       " 'encounters',\n",
       " 'cia',\n",
       " 'annette',\n",
       " 'bening',\n",
       " 'information',\n",
       " 'generally',\n",
       " 'charged',\n",
       " 'problems',\n",
       " 'inside',\n",
       " 'care',\n",
       " 'international',\n",
       " 'realm',\n",
       " 'countries',\n",
       " 'our',\n",
       " 'country',\n",
       " 'therefore',\n",
       " 'finds',\n",
       " 'highly',\n",
       " 'unusual',\n",
       " 'considers',\n",
       " 'territory',\n",
       " 'form',\n",
       " 'professional',\n",
       " 'aware',\n",
       " 'increasingly',\n",
       " 'complicated',\n",
       " 'possibly',\n",
       " 'illegal',\n",
       " 'involving',\n",
       " 'games',\n",
       " 'true',\n",
       " 'pulled',\n",
       " 'dare',\n",
       " 'scenario',\n",
       " 'foreign',\n",
       " 'almost',\n",
       " 'itself',\n",
       " 'turmoil',\n",
       " 'could',\n",
       " 'effective',\n",
       " 'network',\n",
       " 'set',\n",
       " 'america',\n",
       " 'admittedly',\n",
       " 'terror',\n",
       " 'upon',\n",
       " 'excessive',\n",
       " 'believe',\n",
       " 'few',\n",
       " 'cause',\n",
       " 'jump',\n",
       " 'shopping',\n",
       " 'still',\n",
       " 'liked',\n",
       " 'rises',\n",
       " 'above',\n",
       " 'fare',\n",
       " 'bad',\n",
       " 'exploring',\n",
       " 'effects',\n",
       " 'society',\n",
       " 'law',\n",
       " 'order',\n",
       " 'provides',\n",
       " 'possible',\n",
       " 'hate',\n",
       " 'crimes',\n",
       " 'population',\n",
       " 'effort',\n",
       " 'find',\n",
       " 'civil',\n",
       " 'halfway',\n",
       " 'president',\n",
       " 'martial',\n",
       " 'within',\n",
       " 'city',\n",
       " 'americans',\n",
       " 'risk',\n",
       " 'side',\n",
       " 'effect',\n",
       " 'campaign',\n",
       " 'demise',\n",
       " 'social',\n",
       " 'scary',\n",
       " 'goal',\n",
       " 'beneath',\n",
       " 'groups',\n",
       " 'throughout',\n",
       " 'maintains',\n",
       " 'conflict',\n",
       " 'use',\n",
       " 'letter',\n",
       " 'general',\n",
       " 'bruce',\n",
       " 'willis',\n",
       " 'command',\n",
       " 'army',\n",
       " 'mind',\n",
       " 'greater',\n",
       " 'served',\n",
       " 'results',\n",
       " 'fascinating',\n",
       " 'debate',\n",
       " 'powers',\n",
       " 'act',\n",
       " 'philosophy',\n",
       " 'historical',\n",
       " 'examples',\n",
       " 'points',\n",
       " 'sides',\n",
       " 'rather',\n",
       " 'credit',\n",
       " 'screenwriters',\n",
       " 'lawrence',\n",
       " 'wright',\n",
       " 'intelligence',\n",
       " 'audience',\n",
       " 'know',\n",
       " 'tone',\n",
       " 'dialog',\n",
       " 'makes',\n",
       " 'whatever',\n",
       " 'combined',\n",
       " 'ability',\n",
       " 'experience',\n",
       " 'playing',\n",
       " 'authority',\n",
       " 'performance',\n",
       " 'convincing',\n",
       " 'enjoyable',\n",
       " 'recently',\n",
       " 'shares',\n",
       " 'somewhat',\n",
       " 'unfortunate',\n",
       " 'broken',\n",
       " 'allows',\n",
       " 'us',\n",
       " 'little',\n",
       " 'range',\n",
       " 'role',\n",
       " 'standing',\n",
       " 'straight',\n",
       " 'face',\n",
       " 'exists',\n",
       " 'major',\n",
       " 'behind',\n",
       " 'development',\n",
       " 'plays',\n",
       " 'frank',\n",
       " 'calls',\n",
       " 'pride',\n",
       " 'might',\n",
       " 'difficult',\n",
       " 'say',\n",
       " 'touch',\n",
       " 'known',\n",
       " 'antonio',\n",
       " 'sitcom',\n",
       " 'wings',\n",
       " 'gained',\n",
       " 'dramatic',\n",
       " 'abilities',\n",
       " 'wonderful',\n",
       " 'night',\n",
       " 'whereas',\n",
       " 'actor',\n",
       " 'glory',\n",
       " 'courage',\n",
       " 'under',\n",
       " 'fire',\n",
       " 'horner',\n",
       " 'composer',\n",
       " 'found',\n",
       " 'odd',\n",
       " 'chose',\n",
       " 'seven',\n",
       " 'year',\n",
       " 'realized',\n",
       " 'contained',\n",
       " 'record',\n",
       " 'shown',\n",
       " 'either',\n",
       " 'job',\n",
       " ...]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_count(text):\n",
    "    stopwords_list = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "    word_count = {}\n",
    "    for sent in sent_tokenize(text):\n",
    "        for word in word_tokenize(sent):\n",
    "            if word in word_count and word not in stopwords_list and word not in string.punctuation:\n",
    "                word_count[word] += 1\n",
    "            elif word not in word_count and word not in stopwords_list and word not in string.punctuation:\n",
    "                word_count[word] = 1\n",
    "    return word_count\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return [word for sent in sent_tokenize(text) for word in word_tokenize(sent)]\n",
    "\n",
    "def replace_text_stemming(text):\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    stems = [stemmer.stem(word) for word in tokenize_text(text)]\n",
    "    return \" \".join(stems)\n",
    "\n",
    "def get_vector(text, vocabulary):\n",
    "    tokens = tokenize_text(text)\n",
    "    vector = np.zeros(len(vocabulary))\n",
    "    for token in tokens:\n",
    "        if token in vocabulary:\n",
    "            vector[list(vocabulary.keys()).index(token)]+= 1\n",
    "    return vector\n",
    "    \n",
    "def get_features_labels(df, label_column_name):\n",
    "    if 'Unnamed: 0' in df.columns:\n",
    "        df.drop('Unnamed: 0', 1, inplace=True)\n",
    "    labels = df[label_column_name]\n",
    "    train_features = df.drop(label_column_name, 1)\n",
    "    return train_features, labels.to_numpy()\n",
    "\n",
    "def build_vocab(dataset):\n",
    "    text_corpus = \"\"\n",
    "    for i in range(0, dataset.shape[0]):\n",
    "        text_corpus += \" \" + dataset.iloc[i]['Text']\n",
    "    text_corpus = text_corpus.lower()\n",
    "    \n",
    "    stemmed_corpus = replace_text_stemming(text_corpus)\n",
    "    vocabulary = get_word_count(stemmed_corpus)\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logistic():\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The __init__ function initializes the instance attributes for the class. There should be no inputs to this\n",
    "        function at all. However, you can setup whatever instance attributes you would like to initialize for this\n",
    "        class. Below, I have just placed as an example the weights and bias of the logistic function as instance\n",
    "        attributes.\n",
    "        \"\"\"\n",
    "        self.vocabulary = None\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def get_vocab(self, dataset):\n",
    "        self.vocabulary = Pre.build_vocab(dataset)\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "\t    return(1 / (1 + np.exp(-self.z))) \n",
    "    \n",
    "    def feature_extraction(self, data):\n",
    "        \"\"\"\n",
    "        Optional helper method to code the feature extraction function to transform the raw dataset into a processed\n",
    "        dataset to be used in training.\n",
    "        \"\"\"\n",
    "        df = []\n",
    "        for i in range(0, data.shape[0]):\n",
    "            df.append(Pre.get_vector(data.iloc[i]['Text'], self.vocabulary))\n",
    "        return np.array(df)\n",
    "\n",
    "    def logistic_loss(self, predicted_label, true_label):\n",
    "        \"\"\"\n",
    "        Optional helper method to code the loss function.\n",
    "        \"\"\"\n",
    "\n",
    "        return - np.sum(np.dot(true_label, np.log(predicted_label)), np.dot(1-true_label, np.log(1-predicted_label)))\n",
    "\n",
    "    def stochastic_gradient_descent(self, data, error):\n",
    "        \"\"\"\n",
    "        Optional helper method to compute a gradient update for a single point.\n",
    "        \"\"\"\n",
    "\n",
    "        return np.dot(data, error)\n",
    "\n",
    "    def update_weights(self, learning_rate, gradient):\n",
    "        \"\"\"\n",
    "        Optional helper method to update the weights during stochastic gradient descent.\n",
    "        \"\"\"\n",
    "        new_weights = learning_rate*gradient\n",
    "        self.weights -= new_weights\n",
    "\n",
    "    def update_bias(self, learning_rate, error):\n",
    "        \"\"\"\n",
    "        Optional helper method to update the bias during stochastic gradient descent.\n",
    "        \"\"\"\n",
    "        new_bias = np.dot(learning_rate,error)\n",
    "        self.bias -= new_bias\n",
    "\n",
    "    def predict_labels(self, data_point):\n",
    "        \"\"\"\n",
    "        Optional helper method to produce predictions for a single data point\n",
    "        \"\"\"\n",
    "        return np.round(self.sigmoid(np.dot(data_point, self.weights)))\n",
    "\n",
    "    def train(self, labeled_data, learning_rate=0.1, max_epochs=50):\n",
    "        \"\"\"\n",
    "        You must implement this function and it must take in as input data in the form of a pandas dataframe. This\n",
    "        dataframe must have the label of the data points stored in a column called 'Label'. For example, the column\n",
    "        labeled_data['Label'] must return the labels of every data point in the dataset. Additionally, this function\n",
    "        should not return anything.\n",
    "\n",
    "        The hyperparameters for training will be the learning rate and maximum number of epochs. Once you find the\n",
    "        optimal values, update the default values for both the learning rate and max epochs keyword argument.\n",
    "\n",
    "        The goal of this function is to train the logistic function on the labeled data. Feel free to code this\n",
    "        however you want.\n",
    "        \"\"\"\n",
    "        self.get_vocab(labeled_data)\n",
    "        X, y = Pre.get_features_labels(labeled_data, 'Label')\n",
    "        X = self.feature_extraction(X)\n",
    "\n",
    "        bias = np.ones((X.shape[0], 1))\n",
    "        X = np.concatenate((bias, X), axis=1)\n",
    "\n",
    "        # initialize weights\n",
    "        self.weights = np.zeros(X.shape[1])\n",
    "        self.bias = 0\n",
    "\n",
    "        for step in range(0, max_epochs):\n",
    "            for idx, x_feature, target in enumerate(X, y):\n",
    "                scores = np.dot(x_feature, self.weights)\n",
    "                prediction = self.sigmoid(scores)\n",
    "\n",
    "                # Update weights with gradient\n",
    "                output_error_signal = target - prediction\n",
    "                gradient = self.stochastic_gradient_descent(x_feature, output_error_signal)\n",
    "                self.update_weights(learning_rate, gradient)\n",
    "                self.update_bias(learning_rate, output_error_signal)\n",
    "        return\n",
    "\n",
    "    def predict(self, data):\n",
    "        predicted_labels = []\n",
    "        \"\"\"\n",
    "        This function is designed to produce labels on some data input. The only input is the data in the form of a \n",
    "        pandas dataframe. \n",
    "\n",
    "        Finally, you must return the variable predicted_labels which should contain a list of all the \n",
    "        predicted labels on the input dataset. This list should only contain integers  that are either 0 (negative) or 1\n",
    "        (positive) for each data point.\n",
    "\n",
    "        The rest of the implementation can be fully customized.\n",
    "        \"\"\"\n",
    "        X, y = Pre.get_features_labels(data, 'Label')\n",
    "        X = self.feature_extraction(X)\n",
    "\n",
    "        for feature in X:\n",
    "            predicted_labels.append(self.predict_labels(feature))\n",
    "        return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "47a62bc900abaa30fdaba9a8a8bbebde41861c65136e600fc26b837ae5d20c8c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
