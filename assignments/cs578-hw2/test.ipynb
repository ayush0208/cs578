{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\garga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spoiled rich kid kelley morse ( chris klein ) ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the bond series is an island in the film world...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tarzan chad'z = good ) 1999 , g , 90 minutes [...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a frequent error is the categorization of a te...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>part one of \" the strangest movies ever made \"...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Label\n",
       "0  spoiled rich kid kelley morse ( chris klein ) ...      0\n",
       "1  the bond series is an island in the film world...      1\n",
       "2  tarzan chad'z = good ) 1999 , g , 90 minutes [...      1\n",
       "3  a frequent error is the categorization of a te...      1\n",
       "4  part one of \" the strangest movies ever made \"...      1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./data.csv')\n",
    "data.drop('Unnamed: 0', 1, inplace=True)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "  return [word for sent in sent_tokenize(text) for word in word_tokenize(sent)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['Label'].value_counts()\n",
    "\n",
    "stopwords_list = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "def remove_stopwords(text):\n",
    "    words = [word for word in word_tokenize(text) if word.lower() not in stopwords_list]\n",
    "    return \" \".join(words)\n",
    "\n",
    "def get_word_count(text):\n",
    "    word_count = {}\n",
    "    for sent in sent_tokenize(text):\n",
    "        for word in word_tokenize(sent):\n",
    "            if word in word_count and word not in stopwords_list and word not in string.punctuation:\n",
    "                word_count[word] += 1\n",
    "            elif word not in word_count and word not in stopwords_list and word not in string.punctuation:\n",
    "                word_count[word] = 1\n",
    "    return word_count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_corpus = \"\"\n",
    "for i in range(0, data.shape[0]):\n",
    "    text_corpus += \" \" + data.iloc[i]['Text']\n",
    "text_corpus = text_corpus.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.PorterStemmer()\n",
    "def replace_text_stemming(text):\n",
    "  stems = [stemmer.stem(word) for word in tokenize_text(text)]\n",
    "  return \" \".join(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6018307"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_corpus = replace_text_stemming(text_corpus)\n",
    "len(stemmed_corpus)\n",
    "# stemmed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemmed_corpus_without_stopwords = remove_stopwords(stemmed_corpus)\n",
    "# len(stemmed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30332"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = get_word_count(stemmed_corpus)\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(text, vocabulary):\n",
    "    tokens = tokenize_text(text)\n",
    "    vector = np.zeros(len(vocabulary))\n",
    "    for token in tokens:\n",
    "        if token in vocabulary:\n",
    "            vector[list(vocabulary.keys()).index(token)]+= 1\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_labels(df, label_column_name):\n",
    "    if 'Unnamed: 0' in df.columns:\n",
    "        df.drop('Unnamed: 0', 1, inplace=True)\n",
    "    labels = df[label_column_name]\n",
    "    train_features = df.drop(label_column_name, 1)\n",
    "    return train_features, labels.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_first = get_vector(data.iloc[1]['Text'], vocabulary)\n",
    "vector_first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron():\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The __init__ function initializes the instance attributes for the class. There should be no inputs to this\n",
    "        function at all. However, you can setup whatever instance attributes you would like to initialize for this\n",
    "        class. Below, I have just placed as an example the weights and bias of the perceptron as instance attributes.\n",
    "        \"\"\"\n",
    "        self.vocabulary = None\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def build_vocab(self, dataset):\n",
    "        text_corpus = \"\"\n",
    "        for i in range(0, dataset.shape[0]):\n",
    "            text_corpus += \" \" + dataset.iloc[i]['Text']\n",
    "        text_corpus = text_corpus.lower()\n",
    "        \n",
    "        stemmed_corpus = replace_text_stemming(text_corpus)\n",
    "        vocabulary = get_word_count(stemmed_corpus)\n",
    "        self.vocabulary = vocabulary\n",
    "\n",
    "    def feature_extraction(self, data):\n",
    "        \"\"\"\n",
    "        Optional helper method to code the feature extraction function to transform the raw dataset into a processed\n",
    "        dataset to be used in perceptron training.\n",
    "        \"\"\"\n",
    "        df = []\n",
    "        for i in range(0, data.shape[0]):\n",
    "            df.append(get_vector(data.iloc[i]['Text'], self.vocabulary))\n",
    "        return np.array(df)\n",
    "\n",
    "    def sgn_function(self, perceptron_input):\n",
    "        \"\"\"\n",
    "        Optional helper method to code the sign function for the perceptron.\n",
    "        \"\"\"\n",
    "        if perceptron_input <=0: return 0\n",
    "        return 1\n",
    "\n",
    "    def update_weights(self, update, feature):\n",
    "        \"\"\"\n",
    "        Optional helper method to update the weights of the perceptron.\n",
    "        \"\"\"\n",
    "        new_weights = update * feature\n",
    "        self.weights += new_weights\n",
    "\n",
    "    def update_bias(self, update):\n",
    "        \"\"\"\n",
    "        Optional helper method to update the bias of the perceptron.\n",
    "        \"\"\"\n",
    "        self.bias += update\n",
    "\n",
    "    def predict_labels(self, data_point):\n",
    "        \"\"\"\n",
    "        Optional helper method to produce predictions for a single data point.\n",
    "        \"\"\"\n",
    "        prediction = np.dot(data_point, self.weights) + self.bias\n",
    "        return self.sgn_function(prediction)\n",
    "\n",
    "    def train(self, labeled_data, learning_rate=None, max_iter=None):\n",
    "        \"\"\"\n",
    "        You must implement this function and it must take in as input data in the form of a pandas dataframe. This\n",
    "        dataframe must have the label of the data points stored in a column called 'Label'. For example, the column\n",
    "        labeled_data['Label'] must return the labels of every data point in the dataset. Additionally, this function\n",
    "        should not return anything.\n",
    "\n",
    "        The hyperparameters for training will be the learning rate and max number of iterations. Once you find the\n",
    "        optimal values of the hyperparameters, update the default values for each keyword argument to reflect those\n",
    "        values.\n",
    "\n",
    "        The goal of this function is to train the perceptron on the labeled data. Feel free to code this however you\n",
    "        want.\n",
    "        \"\"\"\n",
    "        self.build_vocab(labeled_data)\n",
    "        X, y = get_features_labels(labeled_data, 'Label')\n",
    "        X = self.feature_extraction(X)\n",
    "\n",
    "        # initialize weights\n",
    "        self.weights = np.zeros(X.shape[1])\n",
    "        self.bias = 0\n",
    "\n",
    "        for _ in range(max_iter):\n",
    "            \n",
    "            for idx, x_feature in enumerate(X):\n",
    "                output = np.dot(x_feature, self.weights) + self.bias\n",
    "                y_predicted = self.sgn_function(output)\n",
    "\n",
    "                # Perceptron update rule\n",
    "                update = learning_rate * (y[idx] - y_predicted)\n",
    "                self.update_weights(update, x_feature)\n",
    "                self.update_bias(update)\n",
    "        return\n",
    "\n",
    "    def predict(self, data):\n",
    "        predicted_labels = []\n",
    "        \"\"\"\n",
    "        This function is designed to produce labels on some data input. The first input is the data in the form of a \n",
    "        pandas dataframe. \n",
    "        \n",
    "        Finally, you must return the variable predicted_labels which should contain a list of all the \n",
    "        predicted labels on the input dataset. This list should only contain integers that are either 0 (negative) or 1\n",
    "        (positive) for each data point.\n",
    "        \n",
    "        The rest of the implementation can be fully customized.\n",
    "        \"\"\"\n",
    "        X, y = get_features_labels(data, 'Label')\n",
    "        X = self.feature_extraction(X)\n",
    "\n",
    "        for feat in X:\n",
    "            predicted_labels.append(self.predict_labels(feat))\n",
    "        return predicted_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_perceptron  = Perceptron()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_perceptron.train(dataset, 0.1, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_perceptron = model_perceptron.predict(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels = dataset['Label']\n",
    "predictions_correct = np.array(prediction_perceptron) == true_labels\n",
    "predictions_correct.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv('./test_data.csv')\n",
    "test_pred = model_perceptron.predict(test_data)\n",
    "test_labels = test_data['Label']\n",
    "predictions_correct = np.array(test_pred) == test_labels\n",
    "predictions_correct.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_count(text):\n",
    "    stopwords_list = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "    word_count = {}\n",
    "    for sent in sent_tokenize(text):\n",
    "        for word in word_tokenize(sent):\n",
    "            if word in word_count and word not in stopwords_list and word not in string.punctuation:\n",
    "                word_count[word] += 1\n",
    "            elif word not in word_count and word not in stopwords_list and word not in string.punctuation:\n",
    "                word_count[word] = 1\n",
    "    return word_count\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return [word for sent in sent_tokenize(text) for word in word_tokenize(sent)]\n",
    "\n",
    "def replace_text_stemming(text):\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    stems = [stemmer.stem(word) for word in tokenize_text(text)]\n",
    "    return \" \".join(stems)\n",
    "\n",
    "def get_vector(text, vocabulary):\n",
    "    tokens = tokenize_text(text)\n",
    "    vector = np.zeros(len(vocabulary))\n",
    "    for token in tokens:\n",
    "        if token in vocabulary:\n",
    "            vector[list(vocabulary.keys()).index(token)]+= 1\n",
    "    return vector\n",
    "    \n",
    "def get_features_labels(df, label_column_name):\n",
    "    if 'Unnamed: 0' in df.columns:\n",
    "        df.drop('Unnamed: 0', 1, inplace=True)\n",
    "    labels = df[label_column_name]\n",
    "    train_features = df.drop(label_column_name, 1)\n",
    "    return train_features, labels.to_numpy()\n",
    "\n",
    "def build_vocab(dataset):\n",
    "    text_corpus = \"\"\n",
    "    for i in range(0, dataset.shape[0]):\n",
    "        text_corpus += \" \" + dataset.iloc[i]['Text']\n",
    "    text_corpus = text_corpus.lower()\n",
    "    \n",
    "    stemmed_corpus = replace_text_stemming(text_corpus)\n",
    "    vocabulary = get_word_count(stemmed_corpus)\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logistic():\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The __init__ function initializes the instance attributes for the class. There should be no inputs to this\n",
    "        function at all. However, you can setup whatever instance attributes you would like to initialize for this\n",
    "        class. Below, I have just placed as an example the weights and bias of the logistic function as instance\n",
    "        attributes.\n",
    "        \"\"\"\n",
    "        self.vocabulary = None\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def get_vocab(self, dataset):\n",
    "        self.vocabulary = Pre.build_vocab(dataset)\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "\t    return(1 / (1 + np.exp(-self.z))) \n",
    "    \n",
    "    def feature_extraction(self, data):\n",
    "        \"\"\"\n",
    "        Optional helper method to code the feature extraction function to transform the raw dataset into a processed\n",
    "        dataset to be used in training.\n",
    "        \"\"\"\n",
    "        df = []\n",
    "        for i in range(0, data.shape[0]):\n",
    "            df.append(Pre.get_vector(data.iloc[i]['Text'], self.vocabulary))\n",
    "        return np.array(df)\n",
    "\n",
    "    def logistic_loss(self, predicted_label, true_label):\n",
    "        \"\"\"\n",
    "        Optional helper method to code the loss function.\n",
    "        \"\"\"\n",
    "\n",
    "        return - np.sum(np.dot(true_label, np.log(predicted_label)), np.dot(1-true_label, np.log(1-predicted_label)))\n",
    "\n",
    "    def stochastic_gradient_descent(self, data, error):\n",
    "        \"\"\"\n",
    "        Optional helper method to compute a gradient update for a single point.\n",
    "        \"\"\"\n",
    "\n",
    "        return np.dot(data, error)\n",
    "\n",
    "    def update_weights(self, learning_rate, gradient):\n",
    "        \"\"\"\n",
    "        Optional helper method to update the weights during stochastic gradient descent.\n",
    "        \"\"\"\n",
    "        new_weights = learning_rate*gradient\n",
    "        self.weights -= new_weights\n",
    "\n",
    "    def update_bias(self, learning_rate, error):\n",
    "        \"\"\"\n",
    "        Optional helper method to update the bias during stochastic gradient descent.\n",
    "        \"\"\"\n",
    "        new_bias = np.dot(learning_rate,error)\n",
    "        self.bias -= new_bias\n",
    "\n",
    "    def predict_labels(self, data_point):\n",
    "        \"\"\"\n",
    "        Optional helper method to produce predictions for a single data point\n",
    "        \"\"\"\n",
    "        return np.round(self.sigmoid(np.dot(data_point, self.weights)))\n",
    "\n",
    "    def train(self, labeled_data, learning_rate=0.1, max_epochs=50):\n",
    "        \"\"\"\n",
    "        You must implement this function and it must take in as input data in the form of a pandas dataframe. This\n",
    "        dataframe must have the label of the data points stored in a column called 'Label'. For example, the column\n",
    "        labeled_data['Label'] must return the labels of every data point in the dataset. Additionally, this function\n",
    "        should not return anything.\n",
    "\n",
    "        The hyperparameters for training will be the learning rate and maximum number of epochs. Once you find the\n",
    "        optimal values, update the default values for both the learning rate and max epochs keyword argument.\n",
    "\n",
    "        The goal of this function is to train the logistic function on the labeled data. Feel free to code this\n",
    "        however you want.\n",
    "        \"\"\"\n",
    "        self.get_vocab(labeled_data)\n",
    "        X, y = Pre.get_features_labels(labeled_data, 'Label')\n",
    "        X = self.feature_extraction(X)\n",
    "\n",
    "        bias = np.ones((X.shape[0], 1))\n",
    "        X = np.concatenate((bias, X), axis=1)\n",
    "\n",
    "        # initialize weights\n",
    "        self.weights = np.zeros(X.shape[1])\n",
    "        self.bias = 0\n",
    "\n",
    "        for step in range(0, max_epochs):\n",
    "            for idx, x_feature in enumerate(X):\n",
    "                scores = np.dot(x_feature, self.weights)\n",
    "                prediction = self.sigmoid(scores)\n",
    "\n",
    "                # Update weights with gradient\n",
    "                output_error_signal = y[idx] - prediction\n",
    "                gradient = self.stochastic_gradient_descent(x_feature, output_error_signal)\n",
    "                self.update_weights(learning_rate, gradient)\n",
    "                self.update_bias(learning_rate, output_error_signal)\n",
    "        return\n",
    "\n",
    "    def predict(self, data):\n",
    "        predicted_labels = []\n",
    "        \"\"\"\n",
    "        This function is designed to produce labels on some data input. The only input is the data in the form of a \n",
    "        pandas dataframe. \n",
    "\n",
    "        Finally, you must return the variable predicted_labels which should contain a list of all the \n",
    "        predicted labels on the input dataset. This list should only contain integers  that are either 0 (negative) or 1\n",
    "        (positive) for each data point.\n",
    "\n",
    "        The rest of the implementation can be fully customized.\n",
    "        \"\"\"\n",
    "        X, y = Pre.get_features_labels(data, 'Label')\n",
    "        X = self.feature_extraction(X)\n",
    "\n",
    "        for feature in X:\n",
    "            predicted_labels.append(self.predict_labels(feature))\n",
    "        return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logistic = Logistic()\n",
    "dataset = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_perceptron.train(dataset, 0.1, 50)\n",
    "# prediction_perceptron = model_perceptron.predict(dataset)\n",
    "# true_labels = dataset['Label']\n",
    "# predictions_correct = np.array(prediction_perceptron) == true_labels\n",
    "# predictions_correct.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_perceptron = model_perceptron.predict(dataset)\n",
    "true_labels = dataset['Label']\n",
    "predictions_correct = np.array(prediction_perceptron) == true_labels\n",
    "predictions_correct.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kfold_split(train_set, fold_number):\n",
    "    # train_set = pd.read_csv(input_filename)\n",
    "    # train_set = train_set.sample(random_state=18, frac=1)\n",
    "    # fold_number = 10\n",
    "    fold_size=len(train_set)/fold_number\n",
    "    fold_data_list=[]\n",
    "    for i in range(fold_number):\n",
    "        new_fold=train_set.iloc[int(i*fold_size):int((i+1)*fold_size),:]\n",
    "        fold_data_list.append(new_fold)\n",
    "    return fold_data_list\n",
    "\n",
    "def get_train_test_data(fold_data_list, ind):\n",
    "    test_set = fold_data_list[ind]\n",
    "    rem_set = []\n",
    "    for k, data in enumerate(fold_data_list):\n",
    "        if(k!=ind):\n",
    "            rem_set.append(fold_data_list[k])\n",
    "    # new_train_set = pd.concat(rem_set)\n",
    "    train_set = pd.concat(rem_set)\n",
    "    # new_train_set = new_train_set.sample(random_state=32, frac=fraction)\n",
    "    # new_X_train, new_y_train = q2.get_features_labels(new_train_set)\n",
    "    # new_X_test, new_y_test = q2.get_features_labels(test_set)\n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "You may need to import necessary modules like numpy and pandas. However, you can't use any external\n",
    "libraries such as sci-kit learn, etc. to implement the perceptron and the training of the perceptron.\n",
    "The implementation must be done completely by yourself.\n",
    "\n",
    "We are allowing you to use two packages from nltk for text processing: nltk.stem and nltk.tokenize. You cannot import\n",
    "nltk in general, but we are allowing the use of these two packages only. We will check the code in your programs to\n",
    "make sure this is the case and if other packages in nltk are used then we will deduct points from your assignment.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "This is a Python class meant to represent the perceptron model and any sort of feature processing that you may do. You \n",
    "have a lot of flexibility on how you want to implement the training of the perceptron but below I have listed \n",
    "functionality that should not change:\n",
    "    - Arguments to the __init__ function \n",
    "    - Arguments and return statement of the train function\n",
    "    - Arguments and return statement of the predict function \n",
    "\n",
    "\n",
    "When you want the program (perceptron) to train on a dataset, the train function will only take one input which is the \n",
    "raw copy of the data file as a pandas dataframe. Below, is example code of how this is done:\n",
    "\n",
    "    data = pd.read_csv('data.csv', index_col=0)\n",
    "    model = Perceptron()\n",
    "    model.train(data) # Train the model on data.csv\n",
    "\n",
    "\n",
    "It is assumed when this program is evaluated, the predict function takes one input which is the raw copy of the\n",
    "data file as a pandas dataframe and produce as output the list of predicted labels. Below is example code of how this \n",
    "is done:\n",
    "\n",
    "    data = pd.read_csv('data.csv', index_col=0)\n",
    "    model = Perceptron()\n",
    "    predicted_labels = model.predict(data) # Produce predictions using model on data.csv\n",
    "\n",
    "I have added several optional helper methods for you to use in building the pipeline of training the perceptron. It is\n",
    "up to your discretion on if you want to use them or add your own methods.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import Preprocess as Pre\n",
    "\n",
    "class Perceptron():\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The __init__ function initializes the instance attributes for the class. There should be no inputs to this\n",
    "        function at all. However, you can setup whatever instance attributes you would like to initialize for this\n",
    "        class. Below, I have just placed as an example the weights and bias of the perceptron as instance attributes.\n",
    "        \"\"\"\n",
    "        self.vocabulary = None\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "    \n",
    "    def get_vocab(self, dataset):\n",
    "        self.vocabulary = Pre.build_vocab(dataset)\n",
    "\n",
    "    def feature_extraction(self, data):\n",
    "        \"\"\"\n",
    "        Optional helper method to code the feature extraction function to transform the raw dataset into a processed\n",
    "        dataset to be used in perceptron training.\n",
    "        \"\"\"\n",
    "        df = []\n",
    "        for i in range(0, data.shape[0]):\n",
    "            df.append(Pre.get_vector(data.iloc[i]['Text'], self.vocabulary))\n",
    "        return np.array(df)\n",
    "\n",
    "    def sgn_function(self, perceptron_input):\n",
    "        \"\"\"\n",
    "        Optional helper method to code the sign function for the perceptron.\n",
    "        \"\"\"\n",
    "        if perceptron_input <=0: return 0\n",
    "        return 1\n",
    "\n",
    "    def update_weights(self, update, feature):\n",
    "        \"\"\"\n",
    "        Optional helper method to update the weights of the perceptron.\n",
    "        \"\"\"\n",
    "        new_weights = update * feature\n",
    "        self.weights += new_weights\n",
    "\n",
    "    def update_bias(self, update):\n",
    "        \"\"\"\n",
    "        Optional helper method to update the bias of the perceptron.\n",
    "        \"\"\"\n",
    "        self.bias += update\n",
    "\n",
    "    def predict_labels(self, data_point):\n",
    "        \"\"\"\n",
    "        Optional helper method to produce predictions for a single data point.\n",
    "        \"\"\"\n",
    "        prediction = np.dot(data_point, self.weights) + self.bias\n",
    "        return self.sgn_function(prediction)\n",
    "\n",
    "    def train(self, labeled_data, learning_rate=0.1, max_iter=50):\n",
    "        \"\"\"\n",
    "        You must implement this function and it must take in as input data in the form of a pandas dataframe. This\n",
    "        dataframe must have the label of the data points stored in a column called 'Label'. For example, the column\n",
    "        labeled_data['Label'] must return the labels of every data point in the dataset. Additionally, this function\n",
    "        should not return anything.\n",
    "\n",
    "        The hyperparameters for training will be the learning rate and max number of iterations. Once you find the\n",
    "        optimal values of the hyperparameters, update the default values for each keyword argument to reflect those\n",
    "        values.\n",
    "\n",
    "        The goal of this function is to train the perceptron on the labeled data. Feel free to code this however you\n",
    "        want.\n",
    "        \"\"\"\n",
    "        self.get_vocab(labeled_data)\n",
    "        X, y = Pre.get_features_labels(labeled_data, 'Label')\n",
    "        X = self.feature_extraction(X)\n",
    "\n",
    "        # initialize weights\n",
    "        self.weights = np.zeros(X.shape[1])\n",
    "        self.bias = 0\n",
    "\n",
    "        for _ in range(max_iter):\n",
    "            \n",
    "            for idx, x_feature in enumerate(X):\n",
    "                output = np.dot(x_feature, self.weights) + self.bias\n",
    "                y_predicted = self.sgn_function(output)\n",
    "\n",
    "                # Perceptron update rule\n",
    "                update = learning_rate * (y[idx] - y_predicted)\n",
    "                self.update_weights(update, x_feature)\n",
    "                self.update_bias(update)\n",
    "        return\n",
    "\n",
    "    def predict(self, data):\n",
    "        predicted_labels = []\n",
    "        \"\"\"\n",
    "        This function is designed to produce labels on some data input. The first input is the data in the form of a \n",
    "        pandas dataframe. \n",
    "        \n",
    "        Finally, you must return the variable predicted_labels which should contain a list of all the \n",
    "        predicted labels on the input dataset. This list should only contain integers that are either 0 (negative) or 1\n",
    "        (positive) for each data point.\n",
    "        \n",
    "        The rest of the implementation can be fully customized.\n",
    "        \"\"\"\n",
    "        X, y = Pre.get_features_labels(data, 'Label')\n",
    "        X = self.feature_extraction(X)\n",
    "\n",
    "        for feat in X:\n",
    "            predicted_labels.append(self.predict_labels(feat))\n",
    "        return predicted_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_csv('data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "def remove_between_square_brackets(text):\n",
    "  return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "def remove_special_characters(text, remove_digits=True):\n",
    "  pattern=r'[^a-zA-z0-9\\s]'\n",
    "  text=re.sub(pattern,'',text) \n",
    "  return text\n",
    "\n",
    "def get_word_count(text):\n",
    "    stopwords_list = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "    word_count = {}\n",
    "    for sent in sent_tokenize(text):\n",
    "        for word in word_tokenize(sent):\n",
    "            if word in word_count and word not in stopwords_list and word not in string.punctuation:\n",
    "                word_count[word] += 1\n",
    "            elif word not in word_count and word not in stopwords_list and word not in string.punctuation:\n",
    "                word_count[word] = 1\n",
    "    return word_count\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return [word for sent in sent_tokenize(text) for word in word_tokenize(sent)]\n",
    "\n",
    "def replace_text_stemming(text):\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    stems = [stemmer.stem(word) for word in tokenize_text(text)]\n",
    "    return \" \".join(stems)\n",
    "\n",
    "def get_vector(text, vocabulary):\n",
    "    tokens = tokenize_text(text)\n",
    "    vector = np.zeros(len(vocabulary))\n",
    "    for token in tokens:\n",
    "        if token in vocabulary:\n",
    "            vector[list(vocabulary.keys()).index(token)]+= 1\n",
    "    return vector\n",
    "    \n",
    "def get_features_labels(df, label_column_name):\n",
    "    if 'Unnamed: 0' in df.columns:\n",
    "        df.drop('Unnamed: 0', 1, inplace=True)\n",
    "    labels = df[label_column_name]\n",
    "    train_features = df.drop(label_column_name, 1)\n",
    "    return train_features, labels.to_numpy()\n",
    "\n",
    "def build_vocab(dataset):\n",
    "    text_corpus = \"\"\n",
    "    for i in range(0, dataset.shape[0]):\n",
    "        text_corpus += \" \" + dataset.iloc[i]['Text']\n",
    "    text_corpus = text_corpus.lower()\n",
    "    text_corpus = remove_between_square_brackets(text_corpus)\n",
    "    text_corpus = remove_special_characters(text_corpus, True)\n",
    "    \n",
    "    stemmed_corpus = replace_text_stemming(text_corpus)\n",
    "    vocabulary = get_word_count(stemmed_corpus)\n",
    "    updated_vocab = {}\n",
    "    for key in vocabulary.keys():\n",
    "        if(vocabulary[key]>0.01*dataset.shape[0] and vocabulary[key]<2*dataset.shape[0]):\n",
    "            updated_vocab[key] = vocabulary[key]\n",
    "    return updated_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current learning rate: 0.1\n",
      "Current index : 0\n",
      "\n",
      "Training Accuracy Result!\n",
      "***************\n",
      "Accuracy: 0.8985294117647059\n",
      "***************\n",
      "\n",
      "Testing Accuracy Result!\n",
      "***************\n",
      "Accuracy: 0.7558823529411764\n",
      "***************\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-133-d4d306ba96e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mpredicted_train_labels_perceptron\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mperceptron\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mpredicted_test_labels_perceptron\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mperceptron\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mtrain_accuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted_train_labels_perceptron\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpredicted_test_labels_perceptron\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mtrain_learn_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_accuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mval_learn_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_accuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "perceptron = Perceptron()\n",
    "max_iter_list = [1,5,10,20,50,100]\n",
    "learning_rate_list = [0.01, 0.05, 0.1, 0.2, 0.8]\n",
    "# learning_rate_list = [0.1]\n",
    "fold_data_list = get_kfold_split(all_data, 5)\n",
    "# create a for loop here\n",
    "# for iter in max_iter_list:\n",
    "train_iter_list = []\n",
    "val_iter_list = []\n",
    "for l in learning_rate_list:\n",
    "    print(\"Current learning rate:\", l)\n",
    "    train_learn_list = []\n",
    "    val_learn_list = []\n",
    "    for ind in range(5):\n",
    "        print(\"Current index :\", ind)\n",
    "        train_set, test_set = get_train_test_data(fold_data_list,ind)\n",
    "        perceptron.train(train_set, l, 5)\n",
    "        predicted_train_labels_perceptron = perceptron.predict(train_set)\n",
    "        predicted_test_labels_perceptron = perceptron.predict(test_set)\n",
    "        train_accuracy, val_accuracy = eval(train_set['Label'].tolist(), predicted_train_labels_perceptron, test_set['Label'].tolist(),predicted_test_labels_perceptron)\n",
    "        train_learn_list.append(train_accuracy)\n",
    "        val_learn_list.append(val_accuracy)\n",
    "    train_mean = np.mean(train_learn_list)\n",
    "    val_mean = np.mean(val_learn_list)\n",
    "train_iter_list.append(train_mean)\n",
    "val_iter_list.append(val_mean)\n",
    "plt.scatter(learning_rate_list,val_iter_list)\n",
    "plt.title(\"accuracy vs learning rate curve\")\n",
    "plt.xlabel(\"Learning Rate\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(o_train, p_train, o_val, p_val):\n",
    "    # print('\\nTraining Accuracy Result!')\n",
    "    train_accuracy = accuracy(o_train, p_train)\n",
    "    # print('\\nTesting Accuracy Result!')\n",
    "    val_accuracy = accuracy(o_val, p_val)\n",
    "    return train_accuracy, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(orig, pred):\n",
    "    num = len(orig)\n",
    "    if (num != len(pred)):\n",
    "        print('Error!! Num of labels are not equal.')\n",
    "        return\n",
    "    match = 0\n",
    "    for i in range(len(orig)):\n",
    "        o_label = orig[i]\n",
    "        p_label = pred[i]\n",
    "        if (o_label == p_label):\n",
    "            match += 1\n",
    "    print('***************\\nAccuracy: '+str(float(match) / num)+'\\n***************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-------------Perceptron Performance-------------\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-122-50b3c7c8ac46>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n\\n-------------Perceptron Performance-------------\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# This command also runs the evaluation on the unseen test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m eval(train_data['Label'].tolist(), predicted_train_labels_perceptron, test_data['Label'].tolist(),\n\u001b[0m\u001b[0;32m      7\u001b[0m         predicted_test_labels_perceptron, test_data_unseen['Label'].tolist(), predicted_test_labels_unseen_perceptron)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-------------Perceptron Performance-------------\n",
      "\n",
      "\n",
      "Training Accuracy Result!\n",
      "***************\n",
      "Accuracy: 1.0\n",
      "***************\n",
      "\n",
      "Testing Accuracy Result!\n",
      "***************\n",
      "Accuracy: 0.7794117647058824\n",
      "***************\n"
     ]
    }
   ],
   "source": [
    "print('\\n\\n-------------Perceptron Performance-------------\\n')\n",
    "# This command also runs the evaluation on the unseen test set\n",
    "eval(train_set['Label'].tolist(), predicted_train_labels_perceptron, test_set['Label'].tolist(),\n",
    "        predicted_test_labels_perceptron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "47a62bc900abaa30fdaba9a8a8bbebde41861c65136e600fc26b837ae5d20c8c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
